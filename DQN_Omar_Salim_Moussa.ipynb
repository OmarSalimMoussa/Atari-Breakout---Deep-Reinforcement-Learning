{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Omar Salim Moussa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "90u_tlJuxLQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "Importing python libraries necessary to implement DQN and render environment."
      ]
    },
    {
      "metadata": {
        "id": "NF8V_iMckiZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym[atari]\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import random\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FdqdHdEbxdQF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "Helper functions used to preprocess the raw pixel frames of the environment."
      ]
    },
    {
      "metadata": {
        "id": "MegUbG2fSTTm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def grey(img): # Returns greyscale image\n",
        "  return np.mean(img, axis=2).astype(np.uint8)\n",
        "\n",
        "def resize(img): # Downsamples the size of a frame by half in both axes\n",
        "  img = img/255.0\n",
        "  return img[::2, ::2]\n",
        "\n",
        "def preprocess(img):\n",
        "  return grey(resize(img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSdVeXHpzWub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function that returns the value of epsilon given the iteration/time-step"
      ]
    },
    {
      "metadata": {
        "id": "DGdVr9tnKrNL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def anneal(t, eps_0):\n",
        "  slope = (eps_0-0.1)/(-1000000)\n",
        "  return max(slope*t+eps_0, 0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0GTpSHGJ28H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_environment():\n",
        "  env = gym.make('BreakoutDeterministic-v4')\n",
        "  frame = env.reset()\n",
        "  return env, frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSUrgcIKz_J0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function that returns the index of the maximum value in an numpy array."
      ]
    },
    {
      "metadata": {
        "id": "nbUriXQEaIc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def argmax(action_values):\n",
        "  arg = 0\n",
        "  mx = -100000000\n",
        "  for i in range(4):\n",
        "    if action_values[0][0][i] > mx:\n",
        "      mx = action_values[0][0][i]\n",
        "      arg = i\n",
        "  return arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f4qBeM6W0KvU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function that returns the optimal (greedy) action to take given the current state."
      ]
    },
    {
      "metadata": {
        "id": "-Kkrwv-CLsTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimal_action(Q, state):\n",
        "  mask = np.ones(shape = (1, 4))\n",
        "  action_values = Q.predict([[state], [mask]])\n",
        "  return argmax(action_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tJSkK1N30UON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function that computes the Q value of all possible actions given a state."
      ]
    },
    {
      "metadata": {
        "id": "ZfYmGFNBW2Ie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Q_value(Q, state):\n",
        "  mask = np.ones(shape = (1, 4))\n",
        "  action_values = Q.predict([[state], [mask]])\n",
        "  return np.amax(action_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ImYhZXix0eHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function that transforms negative/positive rewards to -1 or 1."
      ]
    },
    {
      "metadata": {
        "id": "DpZmy4raUql6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform(reward):\n",
        "  return np.sign(reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmYmncaQ1eGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function that constructs the new state given the old state and a new frame. The new state is the new frame concatenated with the most recent three frames from the previous state."
      ]
    },
    {
      "metadata": {
        "id": "9O4kwdo9EsSV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_state(state, new_frame):\n",
        "  new_state = state\n",
        "  for i in range(3):\n",
        "    new_state[:, :, i], new_state[:, :, i+1] = new_state[:, :, i+1], new_state[:, :, i]\n",
        "  new_state[:, :, 3] = new_frame\n",
        "  return new_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAekHFg_yCRb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Memory Buffer for Experience Replay\n",
        "I implemented replay memory as a circular array that supports the following methods:\n",
        "\n",
        "\n",
        "*   Initialize circular array with fixed size.\n",
        "*   Append/overwite empty/oldest memory.\n",
        "*   Return a randomly sampled batch of memories of size n\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AHEAWHOib9oz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Buffer:\n",
        "  def __init__(self, size):\n",
        "    self.data = [None] * (size)\n",
        "    self.end = 0\n",
        "    self.length = 0\n",
        "        \n",
        "  def append(self, element):\n",
        "    self.data[self.end] = element\n",
        "    self.end = (self.end + 1) % len(self.data)\n",
        "    self.length = min(self.length + 1, len(self.data))\n",
        "    \n",
        "  def sample(self, n):\n",
        "    batch = []\n",
        "    for i in range(n):\n",
        "      idx = random.randint(0,self.length-1)\n",
        "      batch.append(self.data[idx])\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZiC0XCvcyrK9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q-Network Definition\n",
        "The definition of the function approximator (Deep Q-Network) as outlined by DeepMind's paper."
      ]
    },
    {
      "metadata": {
        "id": "pPOYJ18_b1EJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def atari_model(n_actions):\n",
        "  \n",
        "  frame_shape = (105, 80, 4)\n",
        "\n",
        "  frames_input = keras.layers.Input(frame_shape, name='frames')\n",
        "  actions_input = keras.layers.Input((1, n_actions), name='mask')\n",
        "    \n",
        "  conv_1 = keras.layers.Conv2D(16, (8, 8), activation='relu', strides=(4, 4))(frames_input)\n",
        "  conv_2 = keras.layers.Conv2D(32, (4, 4), activation='relu', strides=(2, 2))(conv_1)\n",
        "  conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
        "  hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
        "  output = keras.layers.Dense(n_actions)(hidden)\n",
        "  filtered_output = keras.layers.Multiply()([output, actions_input])\n",
        "\n",
        "  Q = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
        "  optimizer = keras.optimizers.RMSprop()\n",
        "  Q.compile(optimizer, loss='mse')\n",
        "  \n",
        "  return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4dFEQ7zN0pFU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Decent/Back Propagation\n",
        "This function samples a batch of 32 random memories, uses the memories to build the feature vector X, and uses the Bellman Identity to compute the values of the labels by feeding the final state of each memory into the Q-Network and adding reward."
      ]
    },
    {
      "metadata": {
        "id": "c_cChR0JTaSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_descent(Q, memory, gamma):\n",
        "  batch = memory.sample(32)\n",
        "  # Create 'Label' vector Y and 'Feature' X with Mask\n",
        "  Y = np.zeros(shape = (32, 1, 4))\n",
        "  X = np.zeros(shape = (32, 105, 80, 4))\n",
        "  MASK = np.zeros(shape = (32, 1, 4))\n",
        "  for i in range(32):\n",
        "    Y[i][0][batch[i][1]] = batch[i][3]\n",
        "    if batch[i][4] == 0:\n",
        "      Y[i][0][batch[i][1]] += gamma*Q_value(Q, batch[i][2])\n",
        "    X[i, :, :, :] = batch[i][0]\n",
        "    MASK[i][0][batch[i][1]] = 1\n",
        "  Q.fit([X, MASK], Y, verbose = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dN8knI3Y1zKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Main Function\n",
        "This function is literal translation of the DQN algorithm pseudocode outlined by DeepMind."
      ]
    },
    {
      "metadata": {
        "id": "DfLY1-Hv46P9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  \n",
        "  n_actions = 4 # For Atari Breakout\n",
        "  eps_0 = 0.8\n",
        "  M = 15 # Number of Episodes\n",
        "  N = 5000000 # Size of Replay Memory\n",
        "  T = 10 # Number of Steps per Episode\n",
        "  gamma = 0.8 # Discount Factor\n",
        "  iteration = 0 # Counter to keep track of iteration count\n",
        "  new_state = np.zeros(shape = (105, 80, 4)) # State of the agent/environment - Last 4 frames\n",
        "  replay_memory = Buffer(N)\n",
        "  Q = atari_model(n_actions) # Initialize Model\n",
        "  env = gym.make('BreakoutDeterministic-v4') # Declare Environment - Atari Breakout with 4-frame skipping\n",
        "  \n",
        "  for episode in range(M):\n",
        "    \n",
        "    # Fill state with first 4 frames - All 4 frames are the initial frame of the environment\n",
        "    frame = preprocess(env.reset())\n",
        "    for i in range(4):\n",
        "      new_state[:, :, i] = frame\n",
        "    \n",
        "    for t in range(T):\n",
        "      state = new_state\n",
        "      \n",
        "      # epsilon-greedy\n",
        "      eps = anneal(iteration, eps_0)\n",
        "      iteration += 1\n",
        "      if random.random() < eps:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = optimal_action(Q, state)\n",
        "      # Get new frame after performing selected action\n",
        "      new_frame, reward, game_over, _ = env.step(action)\n",
        "      \n",
        "      # Construct new_state using the new frame\n",
        "      new_state = get_state(state, preprocess(new_frame))\n",
        "      \n",
        "      # Make tuple of new memory/experience\n",
        "      experience = (state, action, new_state, transform(reward), game_over)\n",
        "      \n",
        "      # Append tuple to replay memory\n",
        "      replay_memory.append(experience)\n",
        "      \n",
        "      gradient_descent(Q, replay_memory, gamma)\n",
        "      \n",
        "      if game_over:\n",
        "        break\n",
        "  return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIL7ziNMYp9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Q = main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "am1qqD9312ZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "Helper function to test the trained model Q. It tests it by running the model on a random episode and redering every frame to visually see progress."
      ]
    },
    {
      "metadata": {
        "id": "Eiqu0CB-eMwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "new_state = np.zeros(shape = (105, 80, 4))\n",
        "frame = preprocess(env.reset())\n",
        "for i in range(4):\n",
        "  new_state[:, :, i] = frame\n",
        "game_over = False\n",
        "while game_over == False:\n",
        "  state = new_state\n",
        "  action = optimal_action(Q, state)\n",
        "  new_frame, reward, game_over, _ = env.step(action)\n",
        "  state = get_state(state, preprocess(new_frame))\n",
        "  screen = env.render(mode = 'rgb_array')\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
